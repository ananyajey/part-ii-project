{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from visualization_new import *\n",
    "\n",
    "import wave\n",
    "import librosa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genres(folderpath):\n",
    "    genre_id = {}\n",
    "\n",
    "    for genre in os.listdir(folderpath):\n",
    "        i = 1\n",
    "        while (genre[:i] in genre_id.values() and i <= len(genre)):\n",
    "            i += 1\n",
    "        genre_id[genre] = genre[:i]\n",
    "    \n",
    "    return genre_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_dict(folderpath, genre_id):\n",
    "    audio_dict = {}\n",
    "\n",
    "    for genre in genre_id:\n",
    "        files = os.listdir(os.path.join(folderpath, genre))\n",
    "        count = 0\n",
    "        for file in files:\n",
    "            filepath = os.path.join(folderpath, genre, file)\n",
    "            id = genre_id[genre] + '{:03d}'.format(count)\n",
    "            count += 1\n",
    "            audio_dict[id] = filepath\n",
    "    \n",
    "    return audio_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(id_list, ratio):\n",
    "    random.shuffle(id_list)\n",
    "    split_index = int(ratio * len(id_list))\n",
    "\n",
    "    train_set = id_list[:split_index]\n",
    "    test_set = id_list[split_index:]\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "def split_k_fold(id_list, n_batches):\n",
    "    random.shuffle(id_list)\n",
    "    batches = []\n",
    "    index = 0\n",
    "    batch_size = math.ceil(len(id_list)/n_batches)\n",
    "    \n",
    "    for i in range(n_batches - 1):\n",
    "        batches.append(id_list[index:index+batch_size])\n",
    "        index += batch_size\n",
    "    batches.append(id_list[index:len(id_list)-1])\n",
    "\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_func(image_type):\n",
    "    if image_type == \"spectrogram\":\n",
    "        return wav_to_spectrogram\n",
    "    elif image_type == \"chromagram\":\n",
    "        return wav_to_chromagram\n",
    "    elif image_type == \"mfcc\":\n",
    "        return wav_to_mfcc\n",
    "    elif image_type == \"cochleagram\":\n",
    "        return wav_to_cochleagram\n",
    "    else:\n",
    "        return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "\n",
    "# Paths\n",
    "audio_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/audio/small\"\n",
    "image_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/images/small\"\n",
    "\n",
    "image_type = \"spectrogram\"\n",
    "to_image = get_image_func(image_type)\n",
    "\n",
    "# Image options\n",
    "clip_length = 10\n",
    "shift_length = 5\n",
    "resize_dim = 129\n",
    "\n",
    "# Model parameters\n",
    "generate = True\n",
    "split_ratio = 0.8\n",
    "k_folds = 5 # Only relevant if cross_validation is True\n",
    "cross_validation = False\n",
    "epochs = 15\n",
    "gen_batch_size = 5 # Only relevant if generate is True\n",
    "model_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(id_list, label_list):\n",
    "    labels = np.array([label_list.index(x[0]) for x in id_list])\n",
    "    labels = labels.reshape((len(labels), 1))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_split_label(id, audio_dict, clip_length, shift_length, resize_dim, label_list, folderpath = None):\n",
    "    '''\n",
    "    If folderpath is None, convert the audio file. If a folderpath is provided, obtain the images from this path.\n",
    "    Returns\n",
    "    -------\n",
    "    ids: list\n",
    "    imgs: numpy array\n",
    "    labels: numpy array\n",
    "    '''\n",
    "    ids = []\n",
    "    imgs = None\n",
    "    labels = None\n",
    "\n",
    "    # CONVERT\n",
    "    if folderpath == None:\n",
    "        filepath = audio_dict[id]\n",
    "        images = to_image(filepath)\n",
    "\n",
    "    # TODO: SPLIT\n",
    "    for i in range(2):\n",
    "        if folderpath == None:\n",
    "            img = images[i]\n",
    "        else:\n",
    "            filepath = os.path.join(folderpath, f\"{id}_ch{i}.png\")\n",
    "            assert(os.path.exists(filepath))\n",
    "            img = Image.open(filepath)\n",
    "        img_dict = split(img, id + \"_ch\" + str(i), clip_length, shift_length, resize_dim)\n",
    "        \n",
    "        # Set/update audio sample id list\n",
    "        ids.extend(list(img_dict.keys()))\n",
    "\n",
    "\n",
    "        # Set/update array of image arrays\n",
    "        if imgs is None:\n",
    "            imgs = np.array(list(img_dict.values()))\n",
    "        else:\n",
    "            imgs = np.append(imgs, np.array(list(img_dict.values())), axis = 0)\n",
    "\n",
    "\n",
    "    # TODO: LABEL\n",
    "    labels = get_labels(ids, label_list=label_list)\n",
    "\n",
    "    return ids, imgs, labels\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITHOUT GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_data(id_list, audio_dict, clip_length, shift_length, resize_dim, label_list, folderpath=None):\n",
    "    ids = None\n",
    "    imgs = None\n",
    "    labels = None\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for id in id_list:\n",
    "\n",
    "        # Convert to images, split, and get labels\n",
    "        split_ids, split_imgs, split_labels = image_split_label(id, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, folderpath=folderpath)\n",
    "        \n",
    "        # Set/update audio sample id list\n",
    "        if ids is None:\n",
    "            ids = split_ids\n",
    "        else:\n",
    "            ids = np.append(ids, split_ids, axis = 0)\n",
    "\n",
    "        # Set/update array of image arrays\n",
    "        if imgs is None:\n",
    "            imgs = np.array(split_imgs)\n",
    "        else:\n",
    "            imgs = np.append(imgs, split_imgs, axis = 0)\n",
    "\n",
    "        # Set/update array of image labels\n",
    "        if labels is None:\n",
    "            labels = split_labels\n",
    "        else:\n",
    "            labels = np.append(labels, split_labels, axis = 0)\n",
    "\n",
    "            #train_imgs.extend(list(img_dict.values()))\n",
    "        #print(count)\n",
    "        count += 1\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = random.sample(range(len(ids)), len(ids))\n",
    "    ids = ids[indices]\n",
    "    imgs = imgs[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    return ids, imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(input_shape, type=1):\n",
    "    if type == 1:\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(5))\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''def createModel2():\n",
    "    model = models.Sequential()\n",
    "    # The first two layers with 32 filters of window size 3x3\n",
    "    #model.add(models.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=x_train[0].shape))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=x_train[0].shape))\n",
    "    \n",
    "    model.add(models.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(models.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(models.Dropout(0.25))\n",
    "\n",
    "    model.add(models.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(models.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(models.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(models.Dropout(0.25))\n",
    "\n",
    "    model.add(models.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(models.models.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(models.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(models.Dropout(0.25))\n",
    "\n",
    "    model.add(models.Flatten())\n",
    "    model.add(models.Dense(512, activation='relu'))\n",
    "    model.add(models.Dropout(0.5))\n",
    "    model.add(models.Dense(models.nClasses, activation='softmax'))\n",
    "    \n",
    "    return model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_id = get_genres(audio_folderpath)\n",
    "genre_list = list(genre_id.keys())\n",
    "label_list = [genre_id[genre] for genre in genre_list]\n",
    "audio_dict = get_audio_dict(audio_folderpath, genre_id)\n",
    "\n",
    "id_list = list(audio_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train, temp_test = split_train_test(id_list=id_list, ratio=split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = False\n",
    "generate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation and not generate:\n",
    "    train_id_list, test_id_list = split_train_test(id_list=id_list, ratio=split_ratio)\n",
    "    #train_id_list, test_id_list = temp_train, temp_test\n",
    "    train_ids, x_train, y_train = return_data(train_id_list, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, folderpath=image_folderpath)\n",
    "    test_ids, x_test, y_test = return_data(test_id_list, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, folderpath=image_folderpath)\n",
    "\n",
    "    model = createModel(type=1, input_shape=x_train[0].shape)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, \n",
    "                        validation_data=(x_test, y_test))\n",
    "\n",
    "    test_similarity = np.zeros((len(label_list), len(label_list)))\n",
    "\n",
    "    test_pred_probs = model.predict(x_test)\n",
    "    test_pred_classes = test_pred_probs.argmax(axis=1)\n",
    "    test_actual = y_test.T[0]\n",
    "    for i in range(len(test_pred_classes)):\n",
    "        test_similarity[test_pred_classes[i]][test_actual[i]] += 1\n",
    "    for i in range(len(label_list)):\n",
    "        test_similarity[:, i] = test_similarity[:, i]/sum(test_similarity[:, i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cross_validation and not generate:\n",
    "    train_val_id_list, test_id_list = split_train_test(id_list, split_ratio)\n",
    "    batches = split_k_fold(train_val_id_list, k_folds)\n",
    "\n",
    "    loss = []\n",
    "    accuracy = []\n",
    "    val_loss = []\n",
    "    val_accuracy = []\n",
    "    val_similarity = np.zeros((len(label_list), len(label_list)))\n",
    "    test_similarity = np.zeros((len(label_list), len(label_list)))\n",
    "    test_ids, x_test, y_test = return_data(test_id_list, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, folderpath=image_folderpath)\n",
    "\n",
    "\n",
    "    for b in range(len(batches)):\n",
    "\n",
    "        # Get data\n",
    "        if b != len(batches) - 1:\n",
    "            train_id_list = batches[0:b] + batches[b+1:]\n",
    "        else:\n",
    "            train_id_list = batches[:-1]\n",
    "        \n",
    "        train_id_list = [item for batch in train_id_list for item in batch]\n",
    "        val_id_list = batches[b]\n",
    "\n",
    "        train_ids, x_train, y_train = return_data(train_id_list, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, folderpath=image_folderpath)\n",
    "        val_ids, x_val, y_val = return_data(val_id_list, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, folderpath=image_folderpath)\n",
    "\n",
    "\n",
    "        # Create model\n",
    "        model = createModel(type=1, input_shape=x_train[0].shape)\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(x_train, y_train, epochs=epochs, \n",
    "                            validation_data=(x_val, y_val))\n",
    "        \n",
    "        # Get results\n",
    "        loss.append(np.array(history.history['loss']))\n",
    "        accuracy.append(np.array(history.history['accuracy']))\n",
    "        val_loss.append(np.array(history.history['val_loss']))\n",
    "        val_accuracy.append(np.array(history.history['val_accuracy']))\n",
    "\n",
    "        # Validation classification results\n",
    "        val_pred_probs = model.predict(x_val)\n",
    "        val_pred_classes = val_pred_probs.argmax(axis=1)\n",
    "        val_actual = y_val.T[0]\n",
    "        for i in range(len(val_pred_classes)):\n",
    "            val_similarity[val_pred_classes[i]][val_actual[i]] += 1\n",
    "\n",
    "        # Test classification results\n",
    "        test_pred_probs = model.predict(x_test)\n",
    "        test_pred_classes = test_pred_probs.argmax(axis=1)\n",
    "        test_actual = y_test.T[0]\n",
    "        for i in range(len(test_pred_classes)):\n",
    "            test_similarity[test_pred_classes[i]][test_actual[i]] += 1\n",
    "\n",
    "    loss = np.array(loss)\n",
    "    accuracy = np.array(accuracy)\n",
    "    val_loss = np.array(val_loss)\n",
    "    val_accuracy = np.array(val_accuracy)\n",
    "\n",
    "    for i in range(len(label_list)):\n",
    "        val_similarity[:, i] = val_similarity[:, i]/sum(val_similarity[:, i])\n",
    "        test_similarity[:, i] = test_similarity[:, i]/sum(test_similarity[:, i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation and not generate:\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation and not generate:\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cross_validation and not generate:\n",
    "    plt.plot(sum(loss)/len(loss), label='Average Training Loss')\n",
    "    plt.plot(sum(val_loss)/len(loss), label='Average Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cross_validation and not generate:\n",
    "    plt.plot(sum(accuracy)/len(accuracy), label='Average Training Accuracy')\n",
    "    plt.plot(sum(val_accuracy)/len(val_accuracy), label='Average Validation Accuracy')\n",
    "    plt.title('Average Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cross_validation and not generate:\n",
    "\n",
    "    for i in range(len(label_list)):\n",
    "        val_similarity[:, i] = val_similarity[:, i]/sum(val_similarity[:, i])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(val_similarity, cmap = \"gray_r\")\n",
    "\n",
    "    labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "    for i in range(1, len(labels) - 1):\n",
    "        labels[i] = genre_list[i - 1]\n",
    "\n",
    "    ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    ax.set_xlabel(\"Actual class\")\n",
    "    ax.set_ylabel(\"Predicted class\")\n",
    "    ax.set_title(\"Validation Classification Matrix\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not generate:\n",
    "    for i in range(len(label_list)):\n",
    "        test_similarity[:, i] = test_similarity[:, i]/sum(test_similarity[:, i])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(test_similarity, cmap = \"gray_r\")\n",
    "\n",
    "    labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "    for i in range(1, len(labels) - 1):\n",
    "        labels[i] = genre_list[i - 1]\n",
    "\n",
    "    ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    ax.set_xlabel(\"Actual class\")\n",
    "    ax.set_ylabel(\"Predicted class\")\n",
    "    ax.set_title(\"Test Classification Matrix\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_save(id_list, audio_dict = audio_dict, folderpath = image_folderpath):\n",
    "    for id in id_list:\n",
    "    # CONVERT\n",
    "        filepath = audio_dict[id]\n",
    "        images = to_image(filepath)\n",
    "\n",
    "        # SAVE\n",
    "        for i in range(2):\n",
    "            images[i].save(os.path.join(folderpath, f\"{id}_ch{i}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(id_list, audio_dict, clip_length, shift_length, resize_dim, label_list, folderpath = None, gen_batch_size = 1, model_batch_size = None, mode = \"train\"):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    id_list\n",
    "        List of song IDs to be converted into images\n",
    "\n",
    "    folderpath\n",
    "        Path sa\n",
    "    \n",
    "    n\n",
    "        Number of songs to convert each iteration\n",
    "    \n",
    "    mode\n",
    "        \"generate\" or \"return\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    imgs\n",
    "        Arrays of converted images\n",
    "\n",
    "    labels\n",
    "        Respective labels for converted images in imgs\n",
    "\n",
    "    '''\n",
    "    out_ids = []\n",
    "    out_imgs = None\n",
    "    out_labels = None\n",
    "    n_count = 0\n",
    "    id_count = 0\n",
    "\n",
    "    if mode == \"train\":\n",
    "    \n",
    "        while True:\n",
    "                \n",
    "            '''\n",
    "            # convert 1 song\n",
    "            # update out_... values\n",
    "            # update bigcount and smallcount\n",
    "            # if smallcount mod n is 0:\n",
    "                # shuffle all 3\n",
    "                # update appropriate id list\n",
    "                # yield id and label values\n",
    "                # set everything to none\n",
    "            \n",
    "            # if bigcount == len(idlist):\n",
    "                # count = 0\n",
    "                # shuffle list?\n",
    "            '''\n",
    "\n",
    "            # convert 1 song\n",
    "            id = id_list[id_count]\n",
    "            #print(id_count, id)\n",
    "            \n",
    "            new_ids, new_imgs, new_labels = image_split_label(id, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, folderpath=folderpath)\n",
    "\n",
    "            out_ids.extend(new_ids)\n",
    "\n",
    "            if out_imgs is None:\n",
    "                out_imgs = new_imgs\n",
    "                out_labels = new_labels\n",
    "            else:\n",
    "                out_imgs = np.append(out_imgs, new_imgs, axis=0)\n",
    "                out_labels = np.append(out_labels, new_labels, axis=0)\n",
    "\n",
    "            # update bigcount and smallcount\n",
    "            n_count += 1\n",
    "            id_count += 1\n",
    "\n",
    "            if n_count % gen_batch_size == 0:\n",
    "                assert(len(out_ids) == len(out_imgs) == len(out_labels)) # sanity check\n",
    "                perm = np.random.permutation(len(out_ids))\n",
    "                out_imgs = out_imgs[perm]\n",
    "                out_labels = out_labels[perm]\n",
    "                if model_batch_size is None:\n",
    "                    yield out_imgs, out_labels\n",
    "                else:\n",
    "                    while len(out_labels) >= model_batch_size:\n",
    "                        yield out_imgs[:model_batch_size], out_labels[:model_batch_size]\n",
    "                        out_imgs = out_imgs[model_batch_size:]\n",
    "                        out_labels = out_labels[model_batch_size:]\n",
    "\n",
    "                out_ids = []\n",
    "                out_imgs = None\n",
    "                out_labels = None\n",
    "            \n",
    "            if id_count == len(id_list):\n",
    "                id_count = 0\n",
    "                random.shuffle(id_list)\n",
    "\n",
    "    if mode == \"test\":\n",
    "        # TODO return just images without shuffling\n",
    "        while id_count < len(id_list):\n",
    "            # convert 1 song\n",
    "            id = id_list[id_count]\n",
    "            print(id_count, id)\n",
    "            \n",
    "            new_ids, new_imgs, new_labels = image_split_label(id, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, folderpath=folderpath)\n",
    "\n",
    "            if out_imgs is None:\n",
    "                out_imgs = new_imgs\n",
    "            else:\n",
    "                out_imgs = np.append(out_imgs, new_imgs, axis=0)\n",
    "\n",
    "            n_count += 1\n",
    "            id_count += 1\n",
    "\n",
    "            if n_count % gen_batch_size == 0:\n",
    "                yield out_imgs\n",
    "\n",
    "                out_imgs = None                \n",
    "            \n",
    "            if id_count == len(id_list):\n",
    "                if out_imgs is not None:\n",
    "                    yield out_imgs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_ids(id_list, audio_dict, clip_length, shift_length):\n",
    "    clip_id_list = []\n",
    "    for id in id_list:\n",
    "        filepath = audio_dict[id]\n",
    "        new_ids = []\n",
    "        with wave.open(filepath, \"rb\") as wav_file:\n",
    "            frame_count = wav_file.getnframes()\n",
    "            sample_rate = wav_file.getframerate()\n",
    "\n",
    "            audio_len = frame_count/sample_rate\n",
    "        for i in range(2):\n",
    "            left = 0.\n",
    "            right = left + clip_length\n",
    "            while right <= audio_len:\n",
    "                new_id = f\"{id}_ch{i}_{left}_{right}\"\n",
    "                new_ids.append(new_id)\n",
    "                left += shift_length\n",
    "                right += shift_length\n",
    "        clip_id_list = clip_id_list + new_ids\n",
    "    \n",
    "    return clip_id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "\n",
    "for id in temp_train:\n",
    "    temp = temp + get_clip_ids(id, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = generate_data(test_id_list, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, folderpath=image_folderpath, gen_batch_size=gen_batch_size, model_batch_size=model_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mnext\u001b[39;49m(temp)[\u001b[39m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[69], line 80\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(id_list, audio_dict, clip_length, shift_length, resize_dim, label_list, folderpath, gen_batch_size, model_batch_size, mode)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(out_labels \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m model_batch_size):\n\u001b[0;32m     79\u001b[0m         \u001b[39myield\u001b[39;00m out_imgs[:model_batch_size], out_labels[:model_batch_size]\n\u001b[1;32m---> 80\u001b[0m         out_imgs \u001b[39m=\u001b[39m out_imgs[model_batch_size:]\n\u001b[0;32m     81\u001b[0m         out_labels \u001b[39m=\u001b[39m out_labels[model_batch_size:]\n\u001b[0;32m     83\u001b[0m out_ids \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[69], line 80\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(id_list, audio_dict, clip_length, shift_length, resize_dim, label_list, folderpath, gen_batch_size, model_batch_size, mode)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(out_labels \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m model_batch_size):\n\u001b[0;32m     79\u001b[0m         \u001b[39myield\u001b[39;00m out_imgs[:model_batch_size], out_labels[:model_batch_size]\n\u001b[1;32m---> 80\u001b[0m         out_imgs \u001b[39m=\u001b[39m out_imgs[model_batch_size:]\n\u001b[0;32m     81\u001b[0m         out_labels \u001b[39m=\u001b[39m out_labels[model_batch_size:]\n\u001b[0;32m     83\u001b[0m out_ids \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(temp)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation and generate:\n",
    "\n",
    "    train_id_list, test_id_list = split_train_test(id_list=id_list, ratio=split_ratio)\n",
    "    #train_id_list, test_id_list = temp_train, temp_test\n",
    "\n",
    "    train_set_len = len(get_clip_ids(train_id_list, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length))\n",
    "    test_set_len = len(get_clip_ids(test_id_list, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length))\n",
    "\n",
    "\n",
    "    train_func = generate_data(train_id_list, folderpath=image_folderpath, gen_batch_size=gen_batch_size, model_batch_size=model_batch_size, mode=\"train\", audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list)\n",
    "    val_func = generate_data(test_id_list, folderpath=image_folderpath, gen_batch_size=gen_batch_size, model_batch_size=model_batch_size, mode=\"train\", audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list)\n",
    "    \n",
    "    input_shape = (resize_dim, resize_dim, 1)\n",
    "\n",
    "    model_gen = createModel(input_shape=input_shape)\n",
    "    # TODO: what if not resized -- how to get dimensions of an \n",
    "\n",
    "    model_gen.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    history_gen = model_gen.fit(train_func,\n",
    "                        steps_per_epoch=int(train_set_len/model_batch_size),\n",
    "                                epochs=20,\n",
    "                                validation_data=val_func,\n",
    "                                validation_steps=int(test_set_len/model_batch_size),\n",
    "                                shuffle=True,\n",
    "                                batch_size=model_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation and generate:\n",
    "    plt.plot(history_gen.history['loss'], label='Training Loss')\n",
    "    plt.plot(history_gen.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation and generate:\n",
    "    plt.plot(history_gen.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history_gen.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MY CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.activation import Tanh, Sigmoid, ReLU\n",
    "from cnn.convolutional import Convolutional\n",
    "from cnn.dense import Dense\n",
    "from cnn.loss import MSE, MSE_prime, binary_cross_entropy, binary_cross_entropy_prime\n",
    "from cnn.nn import MNIST\n",
    "from cnn.reshape import Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\n",
    "t2 = np.array([0, 1, 2, 3])\n",
    "\n",
    "indices = random.sample(range(len(t1)), len(t1))\n",
    "\n",
    "t1 = t1[indices]\n",
    "t2 = t2[indices]\n",
    "\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y, limit):\n",
    "    #zero_index = np.where(y == 0)[0][:limit]\n",
    "    #one_index = np.where(y == 1)[0][:limit]\n",
    "    \n",
    "\n",
    "    #all_indices = np.random.permutation(np.hstack((zero_index, one_index)))\n",
    "\n",
    "    #x, y = x[all_indices], y[all_indices]\n",
    "\n",
    "    #x = (x.reshape(len(x), 1, 28, 28)).astype(\"float32\") / 255\n",
    "\n",
    "    #y = np_utils.to_categorical(y)\n",
    "    #y = y.reshape(len(y), 2, 1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reshape(len(y_train), 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(len(x_train), 1, resize_dim, resize_dim)\n",
    "x_test = x_test.reshape(len(x_test), 1, resize_dim, resize_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_train = y_train.reshape(len(y_train), len(y_train[0]), 1)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_test = y_test.reshape(len(y_test), len(y_test[0]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_indices = random.sample(range(len(x_train)), len(x_train))\n",
    "test_indices = random.sample(range(len(x_test)), len(x_test))\n",
    "\n",
    "x_train = x_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "x_test = x_test[test_indices]\n",
    "y_test = y_test[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [[0, 0], [1, 1], [2, 2]]\n",
    "t2 = [0, 1, 2]\n",
    "\n",
    "zip(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    Convolutional((1, resize_dim, resize_dim), 3, 5),\n",
    "    Sigmoid(),\n",
    "    Reshape((5, resize_dim - 2, resize_dim - 2), (5 * (resize_dim - 2) * (resize_dim - 2), 1)),\n",
    "    Dense(5*(resize_dim - 2)*(resize_dim - 2), 100),\n",
    "    Sigmoid(),\n",
    "    Dense(100, 5),\n",
    "    Sigmoid()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.pooling import MaxPooling\n",
    "# TODO: test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randint(20, size=(4, 4))\n",
    "\n",
    "pool_shape = (2, 2)\n",
    "\n",
    "output = np.zeros((int(arr.shape[0]/pool_shape[0]), int(arr.shape[1]/pool_shape[1])))\n",
    "\n",
    "i, j = 0, 0\n",
    "\n",
    "for i in range(output.shape[0]):\n",
    "    for j in range(output.shape[1]):\n",
    "        temp = np.max(arr[i*pool_shape[0]:(i+1)*pool_shape[0], j*pool_shape[1]:j*pool_shape[1]+2])\n",
    "        output[i][j] = temp#np.max(arr[i*pool_shape[0]:(i+1)*pool_shape[0]][j*pool_shape[1]:(j+1)*pool_shape[1]])\n",
    "\n",
    "print(arr)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for e in range(epochs):\n",
    "    error = 0\n",
    "\n",
    "    temp = zip(x_train, y_train)\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        output = x\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        error += MSE(y, output)\n",
    "\n",
    "        grad = MSE_prime(y, output)\n",
    "        for layer in reversed(network):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "    \n",
    "    error /= len(x_train)\n",
    "    print(f\"{e + 1}/{epochs}, error = {error}\")\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in zip(x_test, y_test):\n",
    "        output = x\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "        print(f\"     pred: {np.argmax(output)}, true: {np.argmax(y)}\")\n",
    "        total += 1\n",
    "        if np.argmax(output) == np.argmax(y):\n",
    "            correct += 1\n",
    "    print(f\"correct: {correct}, total: {total}\\n\\n\")\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = x\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    print(f\"pred: {np.argmax(output)}, true: {np.argmax(y)}\")\n",
    "    total += 1\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        correct += 1\n",
    "print(f\"correct: {correct}, total: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/raw\"\n",
    "image_folderpath = \"C:/Users/anany\\Cambridge\\Part II Project\\data\\images\"\n",
    "split_ratio = 0.75\n",
    "image_type = \"spectrogram\"\n",
    "clip_length = 10\n",
    "shift_length = 500\n",
    "resize_dim = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids\n",
    "# train_imgs\n",
    "# train_labels\n",
    "\n",
    "# test_ids\n",
    "# test_imgs\n",
    "# test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp = train_imgs.reshape((74, 1, 28, 28, 4))\n",
    "\n",
    "temp.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''network = [\n",
    "        Convolutional(input_shape=(1, 28, 28, 4), kernel_size=3, depth=5), # output_shape=()\n",
    "        Sigmoid()\n",
    "    ]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''c = Convolutional(input_shape=(1, 28, 28, 4), kernel_size=3, depth=5), # output_shape=()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "for e in range(epochs):\n",
    "    error = 0\n",
    "\n",
    "    temp = zip(x_train, y_train)\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        output = x\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        error += binary_cross_entropy(y, output)\n",
    "\n",
    "        grad = binary_cross_entropy_prime(y, output)\n",
    "        for layer in reversed(network):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "    \n",
    "    error /= len(x_train)\n",
    "    print(f\"{e + 1}/{epochs}, error = {error}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MNIST()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''arr = np.array([[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]])\n",
    "# 2 3 4\n",
    "\n",
    "new_arr = np.moveaxis(arr, 2, 0)\n",
    "# 4 3 2\n",
    "\n",
    "new_arr[:3]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''arr'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''arr = train_imgs[0]\n",
    "\n",
    "new_arr = np.moveaxis(arr, -1, 0)\n",
    "\n",
    "new_arr'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model_gen.predict(generate_data(test_set, mode=\"predict\"))\n",
    "\n",
    "compare = np.zeros((len(label_list), len(label_list)))\n",
    "y_pred_classes = y_pred_probs.argmax(axis=1)\n",
    "actual = y_test.T[0]\n",
    "\n",
    "for i in range(len(y_pred_classes)):\n",
    "    compare[y_pred_classes[i]][actual[i]] += 1\n",
    "\n",
    "'''for i in range(len(label_list)):\n",
    "    compare[:, i] = compare[:, i]/sum(compare[:, i])'''\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(compare, cmap = \"gray_r\")\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "for i in range(1, len(labels) - 1):\n",
    "    labels[i] = genre_list[i - 1]\n",
    "\n",
    "ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel(\"Actual class\")\n",
    "ax.set_ylabel(\"Predicted class\")\n",
    "ax.set_title(\"Clip Classification Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = generate_data(test_set, mode=\"predict_ids_labels\")\n",
    "t1, t2 = next(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(id):\n",
    "    info = id.split('_')\n",
    "    song_id = info[0]\n",
    "    channel = int(info[1][-1])\n",
    "    start_time = info[2]\n",
    "    end_time = info[3]\n",
    "\n",
    "    filepath = audio_dict[song_id]\n",
    "    y, sr = librosa.load(filepath)\n",
    "\n",
    "    audio_segment = y[int(float(start_time) * sr) : int(float(end_time) * sr)]\n",
    "\n",
    "    return Audio(audio_segment, rate = sr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_vote_dict = {}\n",
    "song_clip_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in list(audio_dict.keys()):\n",
    "    # np.where(np.char.find(test_ids, 'r009') >= 0) ids where 'r009' appears in the string\n",
    "    if id in test_set:\n",
    "        song_clip_dict[id] = test_ids[np.where(np.char.find(test_ids, id) >= 0)]\n",
    "    else:\n",
    "        #song_clip_dict[id] = train_ids[np.where(np.char.find(train_ids, id) >= 0)]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class_onehot = np.eye(len(genre_id.keys()))[y_pred_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_ids)):\n",
    "    id = test_ids[i]\n",
    "    if id not in clip_vote_dict.keys():\n",
    "        clip_vote_dict[id] = np.zeros(len(genre_id.keys()))\n",
    "    clip_vote_dict[id] = clip_vote_dict[id] + y_pred_class_onehot[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_vote_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song_id in test_set:\n",
    "    song_vote_dict[song_id] = np.zeros(len(genre_id.keys()))\n",
    "    for clip_id in song_clip_dict[song_id]:\n",
    "        song_vote_dict[song_id] = song_vote_dict[song_id] + clip_vote_dict[clip_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in song_vote_dict:\n",
    "    print(\"id: {0} pred {1} actual {2}   votes {3}\".format(id, label_list[(np.argmax(song_vote_dict[id]))], id[0], song_vote_dict[id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dict['h004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_similarity = np.zeros((len(label_list), len(label_list)))\n",
    "\n",
    "for id in song_vote_dict:\n",
    "    prediction = np.argmax(song_vote_dict[id])\n",
    "    actual = label_list.index(id[0])\n",
    "    song_similarity[prediction][actual] += 1\n",
    "\n",
    "for i in range(len(label_list)):\n",
    "    if sum(song_similarity[:, i]) >= 0:\n",
    "        song_similarity[:, i] = song_similarity[:, i]/sum(song_similarity[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(song_similarity, cmap = \"gray_r\")\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "for i in range(1, len(labels) - 1):\n",
    "    labels[i] = genre_list[i - 1]\n",
    "\n",
    "ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel(\"Actual class\")\n",
    "ax.set_ylabel(\"Predicted class\")\n",
    "ax.set_title(\"Song Classification Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make a dictionary of songs to song samples\n",
    "make a dictionary of song samples to probabilities\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_classifications(labels, prob_array):\n",
    "    # TODO: get probability distributions for a song\n",
    "    pass\n",
    "    # label list genre list whatever whatever\n",
    "    # do the argmax thing\n",
    "    # output probdis = numpy array of size of genres\n",
    "    # for each:\n",
    "        # vote\n",
    "    # divide by number of samples - maybe dont do this so you can add these for several runss\n",
    "\n",
    "\n",
    "# agrmax output of this to get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def something(id_list, probabilities):\n",
    "    for each song:\n",
    "        # get array ids of samples in song\n",
    "        song_classificatio'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d1e73f74c8ec7a1671e8a50a2cf4a7de1dd968f28e814f9d8c3d6c3acdbc158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
