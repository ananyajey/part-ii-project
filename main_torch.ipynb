{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from pipeline.data_read import *\n",
    "from pipeline.image_prep import *\n",
    "from pipeline.image_prep_torch import *\n",
    "from pipeline.results_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "\n",
    "# Paths\n",
    "dataset = \"large\"\n",
    "\n",
    "if dataset == \"small\":\n",
    "    audio_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/audio/small\"\n",
    "    image_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/images/small\"\n",
    "    csv_filepath = \"C:/Users/anany/Cambridge/Part II Project/data/small.csv\"\n",
    "if dataset == \"large\":\n",
    "    audio_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/audio/large\"\n",
    "    image_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/images/large\"\n",
    "    csv_filepath = \"C:/Users/anany/Cambridge/Part II Project/data/large.csv\"\n",
    "\n",
    "image_type = \"spectrogram\"\n",
    "\n",
    "# Image options\n",
    "clip_length = 10\n",
    "shift_length = 5\n",
    "resize_dim = 129\n",
    "save = False\n",
    "\n",
    "# Model parameters\n",
    "generate = False\n",
    "split_ratio = 0.8\n",
    "k_folds = 5 # Only relevant if cross_validation is True\n",
    "cross_validation = True\n",
    "epochs = 20\n",
    "gen_batch_size = 100 # Only relevant if generate is True\n",
    "model_batch_size = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_id = get_genres(audio_folderpath)\n",
    "genre_list = list(genre_id.keys())\n",
    "label_list = [genre_id[genre] for genre in genre_list]\n",
    "\n",
    "if save:\n",
    "    audio_dict = get_audio_dict(audio_folderpath, genre_id) # TODO: CHANGE TO MAKE??\n",
    "    all_song_ids = list(audio_dict.keys())\n",
    "    convert_and_save(all_song_ids, audio_dict=audio_dict, image_type=image_type, image_folderpath=image_folderpath)\n",
    "    create_csv(csv_filepath=csv_filepath, audio_dict=audio_dict)\n",
    "\n",
    "else:\n",
    "    audio_dict = get_audio_dict_csv(csv_filepath=csv_filepath)\n",
    "    all_song_ids = list(audio_dict.keys())\n",
    "\n",
    "train_song_ids, val_song_ids = split_train_test(id_list=all_song_ids, ratio=split_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gendataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GenDataset(Dataset):\n",
    "    def __init__(self, song_ids, audio_dict, clip_length, shift_length, resize_dim, label_list, image_type=None, image_folderpath=None, gen_batch_size = 1, mode = \"train\"):\n",
    "        # generate_data_parameters\n",
    "        self.song_ids = song_ids\n",
    "        self.audio_dict = audio_dict\n",
    "        self.clip_length = clip_length\n",
    "        self.shift_length = shift_length\n",
    "        self.resize_dim = resize_dim\n",
    "        self.label_list = label_list\n",
    "        self.image_type = image_type\n",
    "        self.image_folderpath = image_folderpath\n",
    "        self.gen_batch_size = gen_batch_size\n",
    "        self.mode = mode\n",
    "\n",
    "        # generator function keep-track\n",
    "        self.gen_func = generate_data_torch(song_ids=self.song_ids, audio_dict=self.audio_dict, clip_length=self.clip_length, shift_length=self.shift_length, resize_dim=self.resize_dim, label_list=self.label_list, image_type=self.image_type, image_folderpath=self.image_folderpath, gen_batch_size = self.gen_batch_size, mode = self.mode)\n",
    "        self.clip_ids = []\n",
    "        self.images = None\n",
    "        self.labels = np.empty((0, 1))\n",
    "        self.image_offset = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        len = 0\n",
    "        for id in self.song_ids:\n",
    "            filepath = self.audio_dict[id]\n",
    "            with wave.open(filepath, \"rb\") as wav_file:\n",
    "                frame_count = wav_file.getnframes()\n",
    "                sample_rate = wav_file.getframerate()\n",
    "                audio_len = frame_count/sample_rate\n",
    "                clip_count = int((audio_len - self.clip_length + self.shift_length)/self.shift_length) * 2\n",
    "                len += clip_count\n",
    "        return len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # reset if index = 0\n",
    "        if index == 0:\n",
    "            self.clip_ids = []\n",
    "            self.images = None\n",
    "            self.labels = np.empty((0, 1))\n",
    "            self.image_offset = 0\n",
    "\n",
    "        # check if imgs is empty\n",
    "        if self.images is None:\n",
    "            # get ids, imgs, labels\n",
    "            ids, imgs, labels = next(self.gen_func)\n",
    "            \n",
    "            # update self values for all these\n",
    "            self.clip_ids.extend(ids)\n",
    "            self.images = imgs\n",
    "            self.labels = np.vstack((self.labels, labels))\n",
    "        \n",
    "        # GETTING TUPLE\n",
    "        # imgs[index-offset]\n",
    "        # labels[index]\n",
    "        out_image = self.images[index - self.image_offset]\n",
    "        out_label = self.labels[index]\n",
    "\n",
    "        # if imgs is last one\n",
    "        if (index - self.image_offset) == (self.images.shape[0] - 1):\n",
    "            # update img_offset\n",
    "            self.image_offset += self.images.shape[0]\n",
    "            # empty imgs\n",
    "            self.images = None\n",
    "\n",
    "        out_image = torch.tensor(out_image).permute(2, 0, 1).float()\n",
    "        out_label = torch.tensor(out_label[0]).long()\n",
    "        \n",
    "        return out_image, out_label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, image_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(64 * (image_dim // 4) * (image_dim // 4), 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 5)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.pool(F.relu(self.conv1(input)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation:\n",
    "\n",
    "    # define model\n",
    "    model = Model(resize_dim)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    # train and validation dataloaders\n",
    "    train_dataset = GenDataset(song_ids=train_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"train\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    val_dataset = GenDataset(song_ids=val_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"test\")\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # empty tensors to keep track of loss, accuracy, predicted classes and actual classes\n",
    "    train_clip_loss = torch.tensor([])\n",
    "    val_clip_loss = torch.tensor([])\n",
    "\n",
    "    train_clip_accuracy = torch.tensor([])\n",
    "    val_clip_accuracy = torch.tensor([])\n",
    "\n",
    "    train_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "    train_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "\n",
    "    val_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "    val_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        print(f\"Epoch {epoch + 1}     \", end=\"\")\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        running_loss_train = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            if epoch == epochs - 1:\n",
    "                train_clip_pred_classes = torch.cat((train_clip_pred_classes, torch.tensor(predicted)))\n",
    "                train_clip_actual_classes = torch.cat((train_clip_actual_classes, torch.tensor(labels)))\n",
    "                '''train_clip_pred_classes.extend(predicted)\n",
    "                train_clip_actual_classes.extend(labels)'''\n",
    "\n",
    "            loss = loss_func(output, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss_train += loss.item()\n",
    "\n",
    "        train_clip_loss = torch.cat((train_clip_loss, torch.tensor([running_loss_train/len(train_dataloader)])), dim=0)\n",
    "        train_clip_accuracy = torch.cat((train_clip_accuracy, torch.tensor([correct_train/total_train])), dim=0)\n",
    "\n",
    "        print(f\"Train Loss {running_loss_train/len(train_dataloader)}    Train Accuracy {correct_train/total_train}     \", end = \"\")\n",
    "\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        running_loss_val = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "                output = model(inputs)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                if epoch == epochs - 1:\n",
    "                    val_clip_pred_classes = torch.cat((val_clip_pred_classes, predicted.clone().detach()))\n",
    "                    val_clip_actual_classes = torch.cat((val_clip_actual_classes, labels.clone().detach()))\n",
    "\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = loss_func(output, labels)\n",
    "                running_loss_val += loss.item()\n",
    "            \n",
    "            val_clip_loss = torch.cat((val_clip_loss, torch.tensor([running_loss_val/len(val_dataloader)])), dim=0)\n",
    "            val_clip_accuracy = torch.cat((val_clip_accuracy, torch.tensor([correct_val/total_val])), dim=0)\n",
    "        \n",
    "        print(f\"Val Loss {running_loss_val/len(val_dataloader)}    Val Accuracy {correct_val/total_val}\")\n",
    "        \n",
    "\n",
    "    train_clip_vote_array = F.one_hot(train_clip_pred_classes, num_classes=len(label_list))\n",
    "    val_clip_vote_array = F.one_hot(val_clip_pred_classes, num_classes=len(label_list))\n",
    "\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1     Train Loss 1.5887630708922318    Train Accuracy 0.3160386473429952     Val Loss 1.555821034481855    Val Accuracy 0.394937674891885\n",
      "Epoch 2     Train Loss 1.4306430494994018    Train Accuracy 0.41619967793880835     Val Loss 1.2081537813675114    Val Accuracy 0.526837954718901\n",
      "Epoch 3     Train Loss 1.1042700284050373    Train Accuracy 0.5405475040257649     Val Loss 1.0207741501612393    Val Accuracy 0.5485881455100483\n",
      "Epoch 4     Train Loss 0.988394482926409    Train Accuracy 0.5997101449275363     Val Loss 0.9346655541803779    Val Accuracy 0.601755278555075\n",
      "Epoch 5     Train Loss 0.8956741936174898    Train Accuracy 0.636231884057971     Val Loss 0.8820001314568325    Val Accuracy 0.6283388450775884\n",
      "Epoch 6     Train Loss 0.8162763853520002    Train Accuracy 0.6728824476650563     Val Loss 0.8786544589613511    Val Accuracy 0.655558382091071\n",
      "Epoch 7     Train Loss 0.775478447287753    Train Accuracy 0.6898872785829308     Val Loss 0.8052700383028364    Val Accuracy 0.6670058509285169\n",
      "Epoch 8     Train Loss 0.729920056833666    Train Accuracy 0.7150402576489533     Val Loss 0.7670781413486939    Val Accuracy 0.6860849656575935\n",
      "Epoch 9     Train Loss 0.7022992246060121    Train Accuracy 0.7214814814814815     Val Loss 0.8550760597104161    Val Accuracy 0.6364792673619944\n",
      "Epoch 10     Train Loss 0.6830845424554375    Train Accuracy 0.7287600644122383     Val Loss 0.7384088288810922    Val Accuracy 0.7030017807173747\n",
      "Epoch 11     Train Loss 0.656968916358712    Train Accuracy 0.7397423510466988     Val Loss 0.7108651094926082    Val Accuracy 0.7303485118290511\n",
      "Epoch 12     Train Loss 0.6342729542147364    Train Accuracy 0.7448953301127215     Val Loss 0.6573734776726432    Val Accuracy 0.7442126685321802\n",
      "Epoch 13     Train Loss 0.6044015572257685    Train Accuracy 0.7648953301127214     Val Loss 0.7178129206820414    Val Accuracy 0.697914016789621\n",
      "Epoch 14     Train Loss 0.6010343265459785    Train Accuracy 0.7637359098228663     Val Loss 0.6851797777273487    Val Accuracy 0.7323836174001527\n",
      "Epoch 15     Train Loss 0.5816369468440233    Train Accuracy 0.7731078904991948     Val Loss 0.732714530300137    Val Accuracy 0.6930806410582548\n",
      "Epoch 16     Train Loss 0.5638992456157225    Train Accuracy 0.7822866344605475     Val Loss 0.7636616229367389    Val Accuracy 0.7028745866191809\n",
      "Epoch 17     Train Loss 0.5471216752342535    Train Accuracy 0.7894363929146538     Val Loss 0.6339158310395915    Val Accuracy 0.748410073772577\n",
      "Epoch 18     Train Loss 0.5335805325257668    Train Accuracy 0.791658615136876     Val Loss 0.7422520527616143    Val Accuracy 0.7162299669295344\n",
      "Epoch 19     Train Loss 0.5100892884877388    Train Accuracy 0.8061835748792271     Val Loss 0.8445845052024455    Val Accuracy 0.6048079369117273\n",
      "Epoch 20     Train Loss 0.4953112797744488    Train Accuracy 0.811658615136876     Val Loss 0.8861367424895109    Val Accuracy 0.6859577715593996\n",
      "Batch 1/5 done.   Test Loss 0.7599166885675482   Test Accuracy 0.7355363393040972\n",
      "Epoch 1     Train Loss 1.5364533984452393    Train Accuracy 0.337617575054761     Val Loss 1.4028587166855975    Val Accuracy 0.3807829181494662\n",
      "Epoch 2     Train Loss 1.1954964711786422    Train Accuracy 0.47967401108104624     Val Loss 1.0834795398925379    Val Accuracy 0.4382308083375699\n",
      "Epoch 3     Train Loss 1.0106024090833694    Train Accuracy 0.5770841386419275     Val Loss 0.9929444031502174    Val Accuracy 0.5086426029486528\n",
      "Epoch 4     Train Loss 0.9060969672536997    Train Accuracy 0.6299445947687153     Val Loss 0.8989967670200801    Val Accuracy 0.6120996441281139\n",
      "Epoch 5     Train Loss 0.826500080518202    Train Accuracy 0.669662414637289     Val Loss 0.878713833667883    Val Accuracy 0.6054905948144382\n",
      "Epoch 6     Train Loss 0.7738883562240247    Train Accuracy 0.696849632779281     Val Loss 0.9666782108143093    Val Accuracy 0.6229028978139298\n",
      "Epoch 7     Train Loss 0.7212244624273907    Train Accuracy 0.7199136709186961     Val Loss 0.8483810798223396    Val Accuracy 0.6633197763091001\n",
      "Epoch 8     Train Loss 0.6998452642734951    Train Accuracy 0.7180775673237985     Val Loss 0.8303752211338984    Val Accuracy 0.6858159633960346\n",
      "Epoch 9     Train Loss 0.667047503618207    Train Accuracy 0.7361164798350728     Val Loss 0.950110648463412    Val Accuracy 0.6248093543467209\n",
      "Epoch 10     Train Loss 0.6492051430023539    Train Accuracy 0.740787269681742     Val Loss 0.8733281956311709    Val Accuracy 0.6596339603457041\n",
      "Epoch 11     Train Loss 0.6150069572837173    Train Accuracy 0.758987243911867     Val Loss 0.6863323789642655    Val Accuracy 0.7269954245043213\n",
      "Epoch 12     "
     ]
    }
   ],
   "source": [
    "#if cross_validation:\n",
    "\n",
    "\n",
    "# split train+val and test, make batches\n",
    "train_val_song_ids, test_song_ids = split_train_test(all_song_ids, split_ratio)\n",
    "batches = split_k_fold(train_val_song_ids, k_folds)\n",
    "\n",
    "# empty tensors to keep track of loss and accuracy\n",
    "train_clip_loss_cross = torch.tensor([]).to(device)\n",
    "train_clip_accuracy_cross = torch.tensor([]).to(device)\n",
    "val_clip_loss_cross = torch.tensor([]).to(device)\n",
    "val_clip_accuracy_cross = torch.tensor([]).to(device)\n",
    "\n",
    "test_clip_accuracy = torch.tensor([]).to(device) # concat this, add whole tensor for this\n",
    "\n",
    "# test dataloader\n",
    "test_dataset = GenDataset(song_ids=test_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"test\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# vote ids and arrays\n",
    "val_clip_vote_ids = []\n",
    "val_clip_vote_array = torch.empty((0, len(genre_list))).to(device)\n",
    "\n",
    "test_clip_vote_ids = get_clip_ids(test_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length)\n",
    "test_clip_vote_array = torch.zeros((len(test_clip_vote_ids), len(genre_list))).to(device)\n",
    "\n",
    "\n",
    "\n",
    "for b in range(len(batches)):\n",
    "\n",
    "    # define model\n",
    "    model = Model(resize_dim)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loss function and optimizer\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    # loss and accuracy - keep track\n",
    "    train_clip_loss = torch.tensor([]).to(device)\n",
    "    train_clip_accuracy = torch.tensor([]).to(device)\n",
    "\n",
    "    val_clip_loss = torch.tensor([]).to(device)\n",
    "    val_clip_accuracy = torch.tensor([]).to(device)\n",
    "\n",
    "    # pred/actual classes\n",
    "    '''train_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "    train_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)'''\n",
    "\n",
    "    \n",
    "\n",
    "    # train/val dataloaders\n",
    "    if b != len(batches) - 1:\n",
    "            train_song_ids = batches[0:b] + batches[b+1:]\n",
    "    else:\n",
    "        train_song_ids = batches[:-1]\n",
    "    \n",
    "    train_song_ids = [item for batch in train_song_ids for item in batch]\n",
    "    val_song_ids = batches[b]\n",
    "\n",
    "    train_dataset = GenDataset(song_ids=train_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"train\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    val_dataset = GenDataset(song_ids=val_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"test\")\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}     \", end=\"\")\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        running_loss_train = 0.0\n",
    "\n",
    "        # forward\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            #backward\n",
    "            loss = loss_func(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "\n",
    "        # update train loss\n",
    "        # update train accuracy\n",
    "        train_clip_loss = torch.cat((train_clip_loss, torch.tensor([running_loss_train/len(train_dataloader)]).to(device)), dim=0)\n",
    "        train_clip_accuracy = torch.cat((train_clip_accuracy, torch.tensor([correct_train/total_train]).to(device)), dim=0)\n",
    "\n",
    "        print(f\"Train Loss {running_loss_train/len(train_dataloader)}    Train Accuracy {correct_train/total_train}     \", end = \"\")\n",
    "\n",
    "\n",
    "        # validation\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        running_loss_val = 0.0\n",
    "\n",
    "        val_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "        val_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "                output = model(inputs)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                if epoch == epochs - 1:\n",
    "                    val_clip_pred_classes = torch.cat((val_clip_pred_classes, predicted.clone().detach()))\n",
    "                    val_clip_actual_classes = torch.cat((val_clip_actual_classes, labels.clone().detach()))\n",
    "\n",
    "                    # update val vote array\n",
    "                    '''new_val_clip_vote_array = F.one_hot(predicted, num_classes=len(label_list)).to(device)\n",
    "                    val_clip_vote_array = torch.cat((val_clip_vote_array, new_val_clip_vote_array), dim=0)'''\n",
    "\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = loss_func(output, labels)\n",
    "                running_loss_val += loss.item()\n",
    "            \n",
    "            # update val loss\n",
    "            val_clip_loss = torch.cat((val_clip_loss, torch.tensor([running_loss_val/len(val_dataloader)]).to(device)), dim=0)\n",
    "            # update val accuracy\n",
    "            val_clip_accuracy = torch.cat((val_clip_accuracy, torch.tensor([correct_val/total_val]).to(device)), dim=0)\n",
    "            # update val vote array\n",
    "            if epoch == epochs - 1:\n",
    "                val_clip_vote_ids.extend(val_dataset.clip_ids)\n",
    "                # print(val_clip_vote_ids)\n",
    "                new_val_clip_vote_array = F.one_hot(val_clip_pred_classes, num_classes=len(label_list)).to(device)\n",
    "                val_clip_vote_array = torch.cat((val_clip_vote_array, new_val_clip_vote_array), dim=0)\n",
    "\n",
    "            print(f\"Val Loss {running_loss_val/len(val_dataloader)}    Val Accuracy {correct_val/total_val}\")\n",
    "    \n",
    "    # val\n",
    "    # update vote ids and array\n",
    "    '''val_clip_vote_ids.extend(val_dataset.clip_ids)\n",
    "    new_val_clip_vote_array = F.one_hot(val_clip_pred_classes, num_classes=len(label_list))\n",
    "    val_clip_vote_array = torch.cat((val_clip_vote_array, new_val_clip_vote_array), dim=0)\n",
    "    '''\n",
    "\n",
    "    # test\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    running_loss_test = 0.0\n",
    "\n",
    "    test_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "    test_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            test_clip_pred_classes = torch.cat((test_clip_pred_classes, predicted.clone().detach()))\n",
    "            test_clip_actual_classes = torch.cat((test_clip_actual_classes, labels.clone().detach()))\n",
    "\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = loss_func(output, labels)\n",
    "            running_loss_test += loss.item()\n",
    "\n",
    "        # update test_accuracy\n",
    "        test_clip_accuracy = torch.cat((test_clip_accuracy, torch.tensor([running_loss_test/len(test_dataloader)]).to(device)), dim=0)\n",
    "\n",
    "        # update test vote array\n",
    "        new_test_clip_vote_array = F.one_hot(test_clip_pred_classes, num_classes=len(label_list)).to(device)\n",
    "        test_clip_vote_array = torch.add(test_clip_vote_array, new_test_clip_vote_array)#, dim=0)\n",
    "\n",
    "        print(f\"Batch {b + 1}/{len(batches)} done.   Test Loss {running_loss_test/len(test_dataloader)}   Test Accuracy {correct_test/total_test}\")\n",
    "    \n",
    "    train_clip_loss_cross = torch.cat((train_clip_loss_cross, train_clip_loss.unsqueeze(0)), dim = 0)\n",
    "    train_clip_accuracy_cross = torch.cat((train_clip_accuracy_cross, train_clip_accuracy.unsqueeze(0)), dim = 0)\n",
    "\n",
    "    val_clip_loss_cross = torch.cat((val_clip_loss_cross, val_clip_loss.unsqueeze(0)), dim = 0)\n",
    "    val_clip_accuracy_cross = torch.cat((val_clip_accuracy_cross, val_clip_accuracy.unsqueeze(0)), dim = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation:\n",
    "    train_clip_confusion = get_confusion_matrix(vote_ids=train_dataset.clip_ids, vote_array=train_clip_vote_array, label_list=label_list)\n",
    "    val_clip_confusion = get_confusion_matrix(vote_ids=val_dataset.clip_ids, vote_array=val_clip_vote_array, label_list=label_list)\n",
    "\n",
    "    norm_train_clip_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_train_clip_confusion[:, i] = train_clip_confusion.clone()[:, i]/sum(train_clip_confusion[:, i])\n",
    "\n",
    "    norm_val_clip_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_val_clip_confusion[:, i] = val_clip_confusion.clone()[:, i]/sum(val_clip_confusion[:, i])\n",
    "\n",
    "if cross_validation:\n",
    "    val_clip_confusion = get_confusion_matrix(vote_ids=val_clip_vote_ids, vote_array=val_clip_vote_array, label_list=label_list)\n",
    "    test_clip_confusion = get_confusion_matrix(vote_ids=test_clip_vote_ids, vote_array=val_clip_vote_array, label_list=label_list)\n",
    "\n",
    "    norm_val_clip_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_val_clip_confusion[:, i] = val_clip_confusion.clone()[:, i]/sum(val_clip_confusion[:, i])\n",
    "\n",
    "    norm_test_clip_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_test_clip_confusion[:, i] = test_clip_confusion.clone()[:, i]/sum(test_clip_confusion[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "\n",
    "if cross_validation:\n",
    "    train_clip_loss = train_clip_loss_cross.clone().cpu().numpy()\n",
    "    train_clip_loss = sum(train_clip_loss)/len(train_clip_loss)\n",
    "\n",
    "    val_clip_loss = val_clip_loss_cross.clone().cpu().numpy()\n",
    "    val_clip_loss = sum(val_clip_loss)/len(val_clip_loss)\n",
    "\n",
    "\n",
    "plt.plot(train_clip_loss, label='Training Loss')\n",
    "plt.plot(val_clip_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Function\n",
    "\n",
    "if cross_validation:\n",
    "    train_clip_accuracy = train_clip_accuracy_cross.clone().cpu().numpy()\n",
    "    train_clip_accuracy = sum(train_clip_accuracy)/len(train_clip_accuracy)\n",
    "\n",
    "    val_clip_accuracy = val_clip_accuracy_cross.clone().cpu().numpy()\n",
    "    val_clip_accuracy = sum(val_clip_accuracy)/len(val_clip_loss)\n",
    "\n",
    "plt.plot(train_clip_accuracy, label='Training Accuracy')\n",
    "plt.plot(val_clip_accuracy, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''if cross_validation:\n",
    "    plt.plot(sum(accuracy)/len(accuracy), label='Average Training Accuracy')\n",
    "    plt.plot(sum(val_accuracy)/len(val_accuracy), label='Average Validation Accuracy')\n",
    "    plt.title('Average Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Clip Confusion Matrix\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(norm_val_clip_confusion)#, cmap = \"gray_r\")\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "for i in range(1, len(labels) - 1):\n",
    "    labels[i] = genre_list[i - 1]\n",
    "\n",
    "ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel(\"Actual class\")\n",
    "ax.set_ylabel(\"Predicted class\")\n",
    "ax.set_title(\"Validation Clip Confusion Matrix\")\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm.set_array(norm_val_clip_confusion)\n",
    "cbar = fig.colorbar(sm, ax=ax)    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Clip Confusion Matrix\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(norm_test_clip_confusion, cmap = \"viridis\")\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "for i in range(1, len(labels) - 1):\n",
    "    labels[i] = genre_list[i - 1]\n",
    "\n",
    "ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel(\"Actual class\")\n",
    "ax.set_ylabel(\"Predicted class\")\n",
    "ax.set_title(\"Test Clip Confusion Matrix\")\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm.set_array(norm_val_clip_confusion)\n",
    "cbar = fig.colorbar(sm, ax=ax)    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cross_validation:\n",
    "    _, test_clip_pred_classes_cross = torch.max(test_clip_vote_array, dim=1)\n",
    "    c = classified_as(test_dataset.clip_ids, test_clip_pred_classes_cross, test_clip_actual_classes, 1, 4)\n",
    "else:\n",
    "    c = classified_as(test_dataset.clip_ids, test_clip_pred_classes, test_clip_actual_classes, 1, 4)\n",
    "\n",
    "if (len(c) != 0):\n",
    "    example = random.randint(0, len(c) - 1)\n",
    "    print(c[example])\n",
    "    print(audio_dict[c[example][:4]])\n",
    "    display(play(c[example], audio_dict=audio_dict))\n",
    "else:\n",
    "    print(\"No such clips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation:\n",
    "    val_song_vote_array = get_song_votes(val_song_ids, val_dataset.clip_ids, val_clip_vote_array.clone())\n",
    "    val_song_pred_classes = val_song_vote_array.argmax(axis=1)\n",
    "    val_song_actual_classes = get_labels(val_song_ids, label_list)\n",
    "\n",
    "    val_song_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(val_song_pred_classes)):\n",
    "        val_song_confusion[val_song_pred_classes[i]][val_song_actual_classes[i]] += 1\n",
    "\n",
    "    norm_val_song_confusion = np.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_val_song_confusion[:, i] = val_song_confusion[:, i]/sum(val_song_confusion[:, i])\n",
    "\n",
    "\n",
    "if cross_validation:\n",
    "    val_song_vote_array = get_song_votes(train_val_song_ids, val_clip_vote_ids, val_clip_vote_array.clone())\n",
    "    val_song_pred_classes = val_song_vote_array.argmax(axis=1)\n",
    "    val_song_actual_classes = get_labels(train_val_song_ids, label_list)\n",
    "\n",
    "    '''print(val_song_pred_classes)\n",
    "\n",
    "    '''\n",
    "    val_song_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(val_song_pred_classes)):\n",
    "        val_song_confusion[val_song_pred_classes[i]][val_song_actual_classes[i]] += 1\n",
    "\n",
    "    norm_val_song_confusion = np.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_val_song_confusion[:, i] = val_song_confusion[:, i]/sum(val_song_confusion[:, i])\n",
    "\n",
    "\n",
    "\n",
    "    test_song_vote_array = get_song_votes(test_song_ids, test_dataset.clip_ids, test_clip_vote_array)\n",
    "    test_song_pred_classes = test_song_vote_array.argmax(axis=1)\n",
    "    test_song_actual_classes = get_labels(test_song_ids, label_list)\n",
    "    \n",
    "    test_song_confusion = np.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(test_song_pred_classes)):\n",
    "        test_song_confusion[test_song_pred_classes[i]][test_song_actual_classes[i]] += 1\n",
    "\n",
    "    norm_test_song_confusion = np.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_test_song_confusion[:, i] = test_song_confusion[:, i]/sum(test_song_confusion[:, i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Song Confusion Matrix\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(norm_val_song_confusion)#, cmap = \"gray_r\")\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "for i in range(1, len(labels) - 1):\n",
    "    labels[i] = genre_list[i - 1]\n",
    "\n",
    "ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel(\"Actual class\")\n",
    "ax.set_ylabel(\"Predicted class\")\n",
    "ax.set_title(\"Validation Song Confusion Matrix\")\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm.set_array(norm_val_song_confusion)\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Song Confusion Matrix\n",
    "\n",
    "if cross_validation:  \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(norm_test_song_confusion, cmap = \"viridis\")\n",
    "\n",
    "    labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "    for i in range(1, len(labels) - 1):\n",
    "        labels[i] = genre_list[i - 1]\n",
    "\n",
    "    ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    ax.set_xlabel(\"Actual class\")\n",
    "    ax.set_ylabel(\"Predicted class\")\n",
    "    ax.set_title(\"Test Song Confusion Matrix\")\n",
    "\n",
    "    sm = cm.ScalarMappable(cmap=\"viridis\")\n",
    "    sm.set_array(norm_val_song_confusion)\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GENRES\")\n",
    "for i in range(len(genre_list)):\n",
    "    print(i, genre_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id_0 = 'b122_ch0_40.0_50.0'\n",
    "sample_id_1 = 'h094_ch0_45.0_55.0'\n",
    "sample_id_2 = 'j011_ch1_50.0_60.0'\n",
    "sample_id_3 = 'p224_ch0_115.0_125.0'\n",
    "sample_id_4 = 'r031_ch1_90.0_100.0'\n",
    "\n",
    "sample_id_0m = 'b044_ch1_60.0_70.0'\n",
    "sample_id_1m = 'h071_ch1_75.0_85.0'\n",
    "sample_id_2m = 'j070_ch1_75.0_85.0'\n",
    "sample_id_3m = 'p124_ch1_135.0_145.0'\n",
    "sample_id_4m = 'r167_ch0_130.0_140.0'\n",
    "\n",
    "sample_id = sample_id_3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_song_id = sample_id[:4]\n",
    "sample_song_ids, sample_img_arrays, sample_labels = return_data(id_list = [sample_song_id], audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type = image_type, image_folderpath=image_folderpath)\n",
    "index = sample_song_ids.index(sample_id)\n",
    "sample_img = sample_img_arrays[index]\n",
    "sample_label = sample_labels[index]\n",
    "\n",
    "sample_img = torch.tensor(sample_img).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
    "sample_label = torch.tensor(sample_label[0]).long().unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_output = (model(sample_img))\n",
    "    sample_pred = torch.max(sample_output.data, 1)[1]\n",
    "\n",
    "\n",
    "print(f\"Predicted genre: {genre_list[sample_pred.item()]}\")\n",
    "print(f\"Actual genre: {genre_list[sample_label.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(sample_id, audio_dict=audio_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
