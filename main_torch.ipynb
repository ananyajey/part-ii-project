{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pipeline.data_read import *\n",
    "from pipeline.image_prep import *\n",
    "from pipeline.image_prep_torch import *\n",
    "from pipeline.results_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "\n",
    "# Paths\n",
    "dataset = \"small\"\n",
    "\n",
    "if dataset == \"small\":\n",
    "    audio_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/audio/small\"\n",
    "    image_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/images/small\"\n",
    "    csv_filepath = \"C:/Users/anany/Cambridge/Part II Project/data/small.csv\"\n",
    "if dataset == \"large\":\n",
    "    audio_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/audio/large\"\n",
    "    image_folderpath = \"C:/Users/anany/Cambridge/Part II Project/data/images/large\"\n",
    "    csv_filepath = \"C:/Users/anany/Cambridge/Part II Project/data/large.csv\"\n",
    "\n",
    "image_type = \"spectrogram\"\n",
    "\n",
    "# Image options\n",
    "clip_length = 10\n",
    "shift_length = 50\n",
    "resize_dim = 30\n",
    "save = False\n",
    "\n",
    "# Model parameters\n",
    "generate = False\n",
    "split_ratio = 0.8\n",
    "k_folds = 5 # Only relevant if cross_validation is True\n",
    "cross_validation = True\n",
    "epochs = 3\n",
    "gen_batch_size = 100 # Only relevant if generate is True\n",
    "model_batch_size = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_id = get_genres(audio_folderpath)\n",
    "genre_list = list(genre_id.keys())\n",
    "label_list = [genre_id[genre] for genre in genre_list]\n",
    "\n",
    "if save:\n",
    "    audio_dict = get_audio_dict(audio_folderpath, genre_id) # TODO: CHANGE TO MAKE??\n",
    "    all_song_ids = list(audio_dict.keys())\n",
    "    convert_and_save(all_song_ids, audio_dict=audio_dict, image_type=image_type, image_folderpath=image_folderpath)\n",
    "    create_csv(csv_filepath=csv_filepath, audio_dict=audio_dict)\n",
    "\n",
    "else:\n",
    "    audio_dict = get_audio_dict_csv(csv_filepath=csv_filepath)\n",
    "    all_song_ids = list(audio_dict.keys())\n",
    "\n",
    "train_song_ids, val_song_ids = split_train_test(id_list=all_song_ids, ratio=split_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gendataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GenDataset(Dataset):\n",
    "    def __init__(self, song_ids, audio_dict, clip_length, shift_length, resize_dim, label_list, image_type=None, image_folderpath=None, gen_batch_size = 1, mode = \"train\"):\n",
    "        # generate_data_parameters\n",
    "        self.song_ids = song_ids\n",
    "        self.audio_dict = audio_dict\n",
    "        self.clip_length = clip_length\n",
    "        self.shift_length = shift_length\n",
    "        self.resize_dim = resize_dim\n",
    "        self.label_list = label_list\n",
    "        self.image_type = image_type\n",
    "        self.image_folderpath = image_folderpath\n",
    "        self.gen_batch_size = gen_batch_size\n",
    "        self.mode = mode\n",
    "\n",
    "        # generator function keep-track\n",
    "        self.gen_func = generate_data_torch(song_ids=self.song_ids, audio_dict=self.audio_dict, clip_length=self.clip_length, shift_length=self.shift_length, resize_dim=self.resize_dim, label_list=self.label_list, image_type=self.image_type, image_folderpath=self.image_folderpath, gen_batch_size = self.gen_batch_size, mode = self.mode)\n",
    "        self.clip_ids = []\n",
    "        self.images = None\n",
    "        self.labels = np.empty((0, 1))\n",
    "        self.image_offset = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        len = 0\n",
    "        for id in self.song_ids:\n",
    "            filepath = self.audio_dict[id]\n",
    "            with wave.open(filepath, \"rb\") as wav_file:\n",
    "                frame_count = wav_file.getnframes()\n",
    "                sample_rate = wav_file.getframerate()\n",
    "                audio_len = frame_count/sample_rate\n",
    "                clip_count = int((audio_len - self.clip_length + self.shift_length)/self.shift_length) * 2\n",
    "                len += clip_count\n",
    "        return len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # reset if index = 0\n",
    "        if index == 0:\n",
    "            self.clip_ids = []\n",
    "            self.images = None\n",
    "            self.labels = np.empty((0, 1))\n",
    "            self.image_offset = 0\n",
    "\n",
    "        # check if imgs is empty\n",
    "        if self.images is None:\n",
    "            # get ids, imgs, labels\n",
    "            ids, imgs, labels = next(self.gen_func)\n",
    "            \n",
    "            # update self values for all these\n",
    "            self.clip_ids.extend(ids)\n",
    "            self.images = imgs\n",
    "            self.labels = np.vstack((self.labels, labels))\n",
    "        \n",
    "        # GETTING TUPLE\n",
    "        # imgs[index-offset]\n",
    "        # labels[index]\n",
    "        out_image = self.images[index - self.image_offset]\n",
    "        out_label = self.labels[index]\n",
    "\n",
    "        # if imgs is last one\n",
    "        if (index - self.image_offset) == (self.images.shape[0] - 1):\n",
    "            # update img_offset\n",
    "            self.image_offset += self.images.shape[0]\n",
    "            # empty imgs\n",
    "            self.images = None\n",
    "\n",
    "        out_image = torch.tensor(out_image).permute(2, 0, 1).float()\n",
    "        out_label = torch.tensor(out_label[0]).long()\n",
    "        \n",
    "        return out_image, out_label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, image_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(64 * (image_dim // 4) * (image_dim // 4), 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 5)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.pool(F.relu(self.conv1(input)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation:\n",
    "\n",
    "    # define model\n",
    "    model = Model(resize_dim)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    # train and validation dataloaders\n",
    "    train_dataset = GenDataset(song_ids=train_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"train\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    val_dataset = GenDataset(song_ids=val_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"test\")\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # empty tensors to keep track of loss, accuracy, predicted classes and actual classes\n",
    "    train_clip_loss = torch.tensor([])\n",
    "    val_clip_loss = torch.tensor([])\n",
    "\n",
    "    train_clip_accuracy = torch.tensor([])\n",
    "    val_clip_accuracy = torch.tensor([])\n",
    "\n",
    "    train_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "    train_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "\n",
    "    val_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "    val_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        print(f\"Epoch {epoch + 1}     \", end=\"\")\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        running_loss_train = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            if epoch == epochs - 1:\n",
    "                train_clip_pred_classes = torch.cat((train_clip_pred_classes, torch.tensor(predicted)))\n",
    "                train_clip_actual_classes = torch.cat((train_clip_actual_classes, torch.tensor(labels)))\n",
    "                '''train_clip_pred_classes.extend(predicted)\n",
    "                train_clip_actual_classes.extend(labels)'''\n",
    "\n",
    "            loss = loss_func(output, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss_train += loss.item()\n",
    "\n",
    "        train_clip_loss = torch.cat((train_clip_loss, torch.tensor([running_loss_train/len(train_dataloader)])), dim=0)\n",
    "        train_clip_accuracy = torch.cat((train_clip_accuracy, torch.tensor([correct_train/total_train])), dim=0)\n",
    "\n",
    "        print(f\"Train Loss {running_loss_train/len(train_dataloader)}    Train Accuracy {correct_train/total_train}     \", end = \"\")\n",
    "\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        running_loss_val = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "                output = model(inputs)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                if epoch == epochs - 1:\n",
    "                    val_clip_pred_classes = torch.cat((val_clip_pred_classes, predicted.clone().detach()))\n",
    "                    val_clip_actual_classes = torch.cat((val_clip_actual_classes, labels.clone().detach()))\n",
    "\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = loss_func(output, labels)\n",
    "                running_loss_val += loss.item()\n",
    "            \n",
    "            val_clip_loss = torch.cat((val_clip_loss, torch.tensor([running_loss_val/len(val_dataloader)])), dim=0)\n",
    "            val_clip_accuracy = torch.cat((val_clip_accuracy, torch.tensor([correct_val/total_val])), dim=0)\n",
    "        \n",
    "        print(f\"Val Loss {running_loss_val/len(val_dataloader)}    Val Accuracy {correct_val/total_val}\")\n",
    "        \n",
    "\n",
    "    train_clip_vote_array = F.one_hot(train_clip_pred_classes, num_classes=len(label_list))\n",
    "    val_clip_vote_array = F.one_hot(val_clip_pred_classes, num_classes=len(label_list))\n",
    "\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if cross_validation:\n",
    "\n",
    "\n",
    "# split train+val and test, make batches\n",
    "train_val_song_ids, test_song_ids = split_train_test(all_song_ids, split_ratio)\n",
    "batches = split_k_fold(train_val_song_ids, k_folds)\n",
    "\n",
    "# empty tensors to keep track of loss and accuracy\n",
    "train_clip_loss_cross = torch.tensor([]).to(device)\n",
    "train_clip_accuracy_cross = torch.tensor([]).to(device)\n",
    "val_clip_loss_cross = torch.tensor([]).to(device)\n",
    "val_clip_accuracy_cross = torch.tensor([]).to(device)\n",
    "\n",
    "test_clip_accuracy = torch.tensor([]).to(device) # concat this, add whole tensor for this\n",
    "\n",
    "# test dataloader\n",
    "test_dataset = GenDataset(song_ids=test_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"test\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# vote ids and arrays\n",
    "val_clip_vote_ids = []\n",
    "val_clip_vote_array = torch.empty((0, len(genre_list))).to(device)\n",
    "\n",
    "test_clip_vote_ids = get_clip_ids(test_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length)\n",
    "test_clip_vote_array = torch.zeros((len(test_clip_vote_ids), len(genre_list))).to(device)\n",
    "\n",
    "\n",
    "\n",
    "for b in range(len(batches)):\n",
    "\n",
    "    # define model\n",
    "    model = Model(resize_dim)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loss function and optimizer\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    # loss and accuracy - keep track\n",
    "    train_clip_loss = torch.tensor([]).to(device)\n",
    "    train_clip_accuracy = torch.tensor([]).to(device)\n",
    "\n",
    "    val_clip_loss = torch.tensor([]).to(device)\n",
    "    val_clip_accuracy = torch.tensor([]).to(device)\n",
    "\n",
    "    # pred/actual classes\n",
    "    '''train_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "    train_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)'''\n",
    "\n",
    "    \n",
    "\n",
    "    # train/val dataloaders\n",
    "    if b != len(batches) - 1:\n",
    "            train_song_ids = batches[0:b] + batches[b+1:]\n",
    "    else:\n",
    "        train_song_ids = batches[:-1]\n",
    "    \n",
    "    train_song_ids = [item for batch in train_song_ids for item in batch]\n",
    "    val_song_ids = batches[b]\n",
    "\n",
    "    train_dataset = GenDataset(song_ids=train_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"train\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    val_dataset = GenDataset(song_ids=val_song_ids, audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type=image_type, image_folderpath=image_folderpath, gen_batch_size=gen_batch_size, mode=\"test\")\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}     \", end=\"\")\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        running_loss_train = 0.0\n",
    "\n",
    "        # forward\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            #backward\n",
    "            loss = loss_func(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "\n",
    "        # update train loss\n",
    "        # update train accuracy\n",
    "        train_clip_loss = torch.cat((train_clip_loss, torch.tensor([running_loss_train/len(train_dataloader)]).to(device)), dim=0)\n",
    "        train_clip_accuracy = torch.cat((train_clip_accuracy, torch.tensor([correct_train/total_train]).to(device)), dim=0)\n",
    "\n",
    "        print(f\"Train Loss {running_loss_train/len(train_dataloader)}    Train Accuracy {correct_train/total_train}     \", end = \"\")\n",
    "\n",
    "\n",
    "        # validation\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        running_loss_val = 0.0\n",
    "\n",
    "        val_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "        val_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "                output = model(inputs)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                if epoch == epochs - 1:\n",
    "                    val_clip_pred_classes = torch.cat((val_clip_pred_classes, predicted.clone().detach()))\n",
    "                    val_clip_actual_classes = torch.cat((val_clip_actual_classes, labels.clone().detach()))\n",
    "\n",
    "                    # update val vote array\n",
    "                    '''new_val_clip_vote_array = F.one_hot(predicted, num_classes=len(label_list)).to(device)\n",
    "                    val_clip_vote_array = torch.cat((val_clip_vote_array, new_val_clip_vote_array), dim=0)'''\n",
    "\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = loss_func(output, labels)\n",
    "                running_loss_val += loss.item()\n",
    "            \n",
    "            # update val loss\n",
    "            val_clip_loss = torch.cat((val_clip_loss, torch.tensor([running_loss_val/len(val_dataloader)]).to(device)), dim=0)\n",
    "            # update val accuracy\n",
    "            val_clip_accuracy = torch.cat((val_clip_accuracy, torch.tensor([correct_val/total_val]).to(device)), dim=0)\n",
    "            # update val vote array\n",
    "            if epoch == epochs - 1:\n",
    "                val_clip_vote_ids.extend(val_dataset.clip_ids)\n",
    "                # print(val_clip_vote_ids)\n",
    "                new_val_clip_vote_array = F.one_hot(val_clip_pred_classes, num_classes=len(label_list)).to(device)\n",
    "                val_clip_vote_array = torch.cat((val_clip_vote_array, new_val_clip_vote_array), dim=0)\n",
    "\n",
    "            print(f\"Val Loss {running_loss_val/len(val_dataloader)}    Val Accuracy {correct_val/total_val}\")\n",
    "    \n",
    "    # val\n",
    "    # update vote ids and array\n",
    "    '''val_clip_vote_ids.extend(val_dataset.clip_ids)\n",
    "    new_val_clip_vote_array = F.one_hot(val_clip_pred_classes, num_classes=len(label_list))\n",
    "    val_clip_vote_array = torch.cat((val_clip_vote_array, new_val_clip_vote_array), dim=0)\n",
    "    '''\n",
    "\n",
    "    # test\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    running_loss_test = 0.0\n",
    "\n",
    "    test_clip_pred_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "    test_clip_actual_classes = torch.tensor([], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            test_clip_pred_classes = torch.cat((test_clip_pred_classes, predicted.clone().detach()))\n",
    "            test_clip_actual_classes = torch.cat((test_clip_actual_classes, labels.clone().detach()))\n",
    "\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = loss_func(output, labels)\n",
    "            running_loss_test += loss.item()\n",
    "\n",
    "        # update test_accuracy\n",
    "        test_clip_accuracy = torch.cat((test_clip_accuracy, torch.tensor([running_loss_test/len(test_dataloader)]).to(device)), dim=0)\n",
    "\n",
    "        # update test vote array\n",
    "        new_test_clip_vote_array = F.one_hot(test_clip_pred_classes, num_classes=len(label_list)).to(device)\n",
    "        test_clip_vote_array = torch.add(test_clip_vote_array, new_test_clip_vote_array)#, dim=0)\n",
    "\n",
    "        print(f\"Batch {b + 1}/{len(batches)} done.   Test Accuracy {running_loss_test/len(test_dataloader)}\")\n",
    "    \n",
    "    train_clip_loss_cross = torch.cat((train_clip_loss_cross, train_clip_loss.unsqueeze(0)), dim = 0)\n",
    "    train_clip_accuracy_cross = torch.cat((train_clip_accuracy_cross, train_clip_accuracy.unsqueeze(0)), dim = 0)\n",
    "\n",
    "    val_clip_loss_cross = torch.cat((val_clip_loss_cross, val_clip_loss.unsqueeze(0)), dim = 0)\n",
    "    val_clip_accuracy_cross = torch.cat((val_clip_accuracy_cross, val_clip_accuracy.unsqueeze(0)), dim = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation:\n",
    "    train_clip_confusion = get_confusion_matrix(vote_ids=train_dataset.clip_ids, vote_array=train_clip_vote_array, label_list=label_list)\n",
    "    val_clip_confusion = get_confusion_matrix(vote_ids=val_dataset.clip_ids, vote_array=val_clip_vote_array, label_list=label_list)\n",
    "\n",
    "    norm_train_clip_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_train_clip_confusion[:, i] = train_clip_confusion.clone()[:, i]/sum(train_clip_confusion[:, i])\n",
    "\n",
    "    norm_val_clip_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_val_clip_confusion[:, i] = val_clip_confusion.clone()[:, i]/sum(val_clip_confusion[:, i])\n",
    "\n",
    "if cross_validation:\n",
    "    val_clip_confusion = get_confusion_matrix(vote_ids=val_clip_vote_ids, vote_array=val_clip_vote_array, label_list=label_list)\n",
    "    test_clip_confusion = get_confusion_matrix(vote_ids=test_clip_vote_ids, vote_array=val_clip_vote_array, label_list=label_list)\n",
    "\n",
    "    norm_val_clip_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_val_clip_confusion[:, i] = val_clip_confusion.clone()[:, i]/sum(val_clip_confusion[:, i])\n",
    "\n",
    "    norm_test_clip_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_test_clip_confusion[:, i] = test_clip_confusion.clone()[:, i]/sum(test_clip_confusion[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp = train_clip_loss_cross.clone().cpu().numpy()\n",
    "\n",
    "plt.plot(sum(temp)/len(temp), label='Average Training Loss')\n",
    "#plt.plot(sum(val_clip_loss_cross)/len(val_clip_loss_cross), label='Average Validation Loss')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "\n",
    "if cross_validation:\n",
    "    train_clip_loss = train_clip_loss_cross.clone().cpu().numpy()\n",
    "    train_clip_loss = sum(train_clip_loss)/len(train_clip_loss)\n",
    "\n",
    "    val_clip_loss = val_clip_loss_cross.clone().cpu().numpy()\n",
    "    val_clip_loss = sum(val_clip_loss)/len(val_clip_loss)\n",
    "\n",
    "\n",
    "plt.plot(train_clip_loss, label='Training Loss')\n",
    "plt.plot(val_clip_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Function\n",
    "\n",
    "if cross_validation:\n",
    "    train_clip_accuracy = train_clip_accuracy_cross.clone().cpu().numpy()\n",
    "    train_clip_accuracy = sum(train_clip_accuracy)/len(train_clip_accuracy)\n",
    "\n",
    "    val_clip_accuracy = val_clip_accuracy_cross.clone().cpu().numpy()\n",
    "    val_clip_accuracy = sum(val_clip_accuracy)/len(val_clip_loss)\n",
    "\n",
    "plt.plot(train_clip_accuracy, label='Training Accuracy')\n",
    "plt.plot(val_clip_accuracy, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''if cross_validation:\n",
    "    plt.plot(sum(accuracy)/len(accuracy), label='Average Training Accuracy')\n",
    "    plt.plot(sum(val_accuracy)/len(val_accuracy), label='Average Validation Accuracy')\n",
    "    plt.title('Average Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Clip Confusion Matrix\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(norm_val_clip_confusion)#, cmap = \"gray_r\")\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "for i in range(1, len(labels) - 1):\n",
    "    labels[i] = genre_list[i - 1]\n",
    "\n",
    "ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel(\"Actual class\")\n",
    "ax.set_ylabel(\"Predicted class\")\n",
    "ax.set_title(\"Validation Clip Confusion Matrix\")\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm.set_array(norm_val_clip_confusion)\n",
    "cbar = fig.colorbar(sm, ax=ax)    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Clip Confusion Matrix\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(norm_test_clip_confusion, cmap = \"viridis\")\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "for i in range(1, len(labels) - 1):\n",
    "    labels[i] = genre_list[i - 1]\n",
    "\n",
    "ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel(\"Actual class\")\n",
    "ax.set_ylabel(\"Predicted class\")\n",
    "ax.set_title(\"Test Clip Confusion Matrix\")\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm.set_array(norm_val_clip_confusion)\n",
    "cbar = fig.colorbar(sm, ax=ax)    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cross_validation:\n",
    "    _, test_clip_pred_classes_cross = torch.max(test_clip_vote_array, dim=1)\n",
    "    c = classified_as(test_dataset.clip_ids, test_clip_pred_classes_cross, test_clip_actual_classes, 1, 4)\n",
    "else:\n",
    "    c = classified_as(test_dataset.clip_ids, test_clip_pred_classes, test_clip_actual_classes, 1, 4)\n",
    "\n",
    "if (len(c) != 0):\n",
    "    example = random.randint(0, len(c) - 1)\n",
    "    print(c[example])\n",
    "    print(audio_dict[c[example][:4]])\n",
    "    display(play(c[example], audio_dict=audio_dict))\n",
    "else:\n",
    "    print(\"No such clips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cross_validation:\n",
    "    val_song_vote_array = get_song_votes(val_song_ids, val_dataset.clip_ids, val_clip_vote_array.clone())\n",
    "    val_song_pred_classes = val_song_vote_array.argmax(axis=1)\n",
    "    val_song_actual_classes = get_labels(val_song_ids, label_list)\n",
    "\n",
    "    val_song_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(val_song_pred_classes)):\n",
    "        val_song_confusion[val_song_pred_classes[i]][val_song_actual_classes[i]] += 1\n",
    "\n",
    "    norm_val_song_confusion = np.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_val_song_confusion[:, i] = val_song_confusion[:, i]/sum(val_song_confusion[:, i])\n",
    "\n",
    "\n",
    "if cross_validation:\n",
    "    val_song_vote_array = get_song_votes(train_val_song_ids, val_clip_vote_ids, val_clip_vote_array.clone())\n",
    "    val_song_pred_classes = val_song_vote_array.argmax(axis=1)\n",
    "    val_song_actual_classes = get_labels(train_val_song_ids, label_list)\n",
    "\n",
    "    '''print(val_song_pred_classes)\n",
    "\n",
    "    '''\n",
    "    val_song_confusion = torch.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(val_song_pred_classes)):\n",
    "        val_song_confusion[val_song_pred_classes[i]][val_song_actual_classes[i]] += 1\n",
    "\n",
    "    norm_val_song_confusion = np.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_val_song_confusion[:, i] = val_song_confusion[:, i]/sum(val_song_confusion[:, i])\n",
    "\n",
    "\n",
    "\n",
    "    test_song_vote_array = get_song_votes(test_song_ids, test_dataset.clip_ids, test_clip_vote_array)\n",
    "    test_song_pred_classes = test_song_vote_array.argmax(axis=1)\n",
    "    test_song_actual_classes = get_labels(test_song_ids, label_list)\n",
    "    \n",
    "    test_song_confusion = np.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(test_song_pred_classes)):\n",
    "        test_song_confusion[test_song_pred_classes[i]][test_song_actual_classes[i]] += 1\n",
    "\n",
    "    norm_test_song_confusion = np.zeros((len(label_list), len(label_list)))\n",
    "    for i in range(len(label_list)):\n",
    "        norm_test_song_confusion[:, i] = test_song_confusion[:, i]/sum(test_song_confusion[:, i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Song Confusion Matrix\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(norm_val_song_confusion)#, cmap = \"gray_r\")\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "for i in range(1, len(labels) - 1):\n",
    "    labels[i] = genre_list[i - 1]\n",
    "\n",
    "ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel(\"Actual class\")\n",
    "ax.set_ylabel(\"Predicted class\")\n",
    "ax.set_title(\"Validation Song Confusion Matrix\")\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm.set_array(norm_val_song_confusion)\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Song Confusion Matrix\n",
    "\n",
    "if cross_validation:  \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(norm_test_song_confusion, cmap = \"viridis\")\n",
    "\n",
    "    labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "    for i in range(1, len(labels) - 1):\n",
    "        labels[i] = genre_list[i - 1]\n",
    "\n",
    "    ax.set_xticklabels(labels, rotation = 45, ha = \"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    ax.set_xlabel(\"Actual class\")\n",
    "    ax.set_ylabel(\"Predicted class\")\n",
    "    ax.set_title(\"Test Song Confusion Matrix\")\n",
    "\n",
    "    sm = cm.ScalarMappable(cmap=\"viridis\")\n",
    "    sm.set_array(norm_val_song_confusion)\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GENRES\")\n",
    "for i in range(len(genre_list)):\n",
    "    print(i, genre_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id_0 = 'b122_ch0_40.0_50.0'\n",
    "sample_id_1 = 'h094_ch0_45.0_55.0'\n",
    "sample_id_2 = 'j011_ch1_50.0_60.0'\n",
    "sample_id_3 = 'p224_ch0_115.0_125.0'\n",
    "sample_id_4 = 'r031_ch1_90.0_100.0'\n",
    "\n",
    "sample_id_0m = 'b044_ch1_60.0_70.0'\n",
    "sample_id_1m = 'h071_ch1_75.0_85.0'\n",
    "sample_id_2m = 'j070_ch1_75.0_85.0'\n",
    "sample_id_3m = 'p124_ch1_135.0_145.0'\n",
    "sample_id_4m = 'r167_ch0_130.0_140.0'\n",
    "\n",
    "sample_id = sample_id_3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_song_id = sample_id[:4]\n",
    "sample_song_ids, sample_img_arrays, sample_labels = return_data(id_list = [sample_song_id], audio_dict=audio_dict, clip_length=clip_length, shift_length=shift_length, resize_dim=resize_dim, label_list=label_list, image_type = image_type, image_folderpath=image_folderpath)\n",
    "index = sample_song_ids.index(sample_id)\n",
    "sample_img = sample_img_arrays[index]\n",
    "sample_label = sample_labels[index]\n",
    "\n",
    "sample_img = torch.tensor(sample_img).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
    "sample_label = torch.tensor(sample_label[0]).long().unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_output = (model(sample_img))\n",
    "    sample_pred = torch.max(sample_output.data, 1)[1]\n",
    "\n",
    "\n",
    "print(f\"Predicted genre: {genre_list[sample_pred.item()]}\")\n",
    "print(f\"Actual genre: {genre_list[sample_label.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(sample_id, audio_dict=audio_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
